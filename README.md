# 10_projects_AI
10 projects to understand AI neural networks
AI Learning Roadmap

**Section 1: Foundations (Math, Regression, Classification, XOR)**

These four projects build the mental model of linear algebra → optimization → nonlinearity.


1. Single‑Feature Linear Regression

- r, R², OLS
- Geometry of projection
- MSE, gradients, updates


2. Multi‑Feature Regression

- Vectorized dot products
- Weight vector as a direction in feature space
- Plane instead of line
- Residual vectors

3. Binary Classification

- Logistic regression
- Sigmoid
- Cross‑entropy
- Decision boundary geometry

4. Neural Network for XOR

- Hidden layer
- Activation functions
Geometry of separating non‑linearly separable data

**Section 2: Sequence Models (RNN → LSTM → Proto‑LLM)**

This is where you transition from static vectors to temporal vectors.


5. Simple RNN: Next Character Predictor

- Recurrence
- Exploding/vanishing gradients
- How memory works

6. LSTM: Better Sequence Memory

- Gates
- Cell state
- Why LSTMs solved long‑range dependencies

7. Translation Mini‑Model (English → Spanish)

- Sequence‑to‑sequence
- Encoder/decoder
- Attention (optional preview)

**Section 3: Agentic Systems (Local Models → LangGraph → RAG)**

This is where you connect neural nets to real‑world systems.

8. How an AI Agent Works Without a Framework

- Run models offline
- AI Agent from scratch
- tool calls
- User inputs
- Memory

9. LangGraph Framework for Agent Workflow

- Tools
- Memory
- State machines

10. RAG System

- Embeddings
- Vector search
- Retrieval
- Context injection

**Neural Networks Simple Math**