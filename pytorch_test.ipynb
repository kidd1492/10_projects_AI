{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "945d4899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b6515",
   "metadata": {},
   "source": [
    "Imports:\n",
    "- torch.nn as nn [PyTorch docs :torch.nn ](https://docs.pytorch.org/docs/stable/nn.html)\n",
    "- torch.optim [PyTorch docs](https://docs.pytorch.org/docs/stable/optim.html#module-torch.optim)\n",
    "    - a package implementing various optimization algorithms.\n",
    "\n",
    "Containers :\n",
    "- nn.Module : Base class for all neural network modules.\n",
    "- nn.Sequential() : [pytorch doc :Sequential](https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential)\n",
    "\n",
    "Non-Linear Activations :\n",
    "- nn.ReLU\n",
    "- nn.Sigmoid\n",
    "\n",
    "Loss Functions :\n",
    "- nn.BCELoss() : Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities:\n",
    "- nn.BCEWithLogitsLoss() : This loss combines a Sigmoid layer and the BCELoss in one single class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09661060",
   "metadata": {},
   "source": [
    "# torch.tensor([])\n",
    "\n",
    "```\n",
    "X = torch.tensor([\n",
    "X = np.array([                      \n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "], dtype=np.float32)\n",
    "\n",
    "y = torch.tensor([\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "], dtype=np.float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "39ed41f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "X = torch.tensor([\n",
    "    [0., 0.],\n",
    "    [0., 1.],\n",
    "    [1., 0.],\n",
    "    [1., 1.]\n",
    "])\n",
    "\n",
    "y = torch.tensor([\n",
    "    [0.],\n",
    "    [1.],\n",
    "    [1.],\n",
    "    [0.]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d799166",
   "metadata": {},
   "source": [
    "Since the BCEWithLogitsLoss() has a built in sigmoid layer we can leave the sigmoid out of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "ad9a70d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XORNet_simple(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=3, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=3, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class XORNet_simple(nn.Module):  #nn.Module Base class for all neural network modules\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 3),     # Input → Hidden\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3, 1),     # Hidden → Output\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "model = XORNet_simple()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a7e32584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'params': [Parameter containing:\n",
      "tensor([[ 2.2627, -1.9063],\n",
      "        [-4.0453,  4.0453],\n",
      "        [-1.9786,  2.4116]], requires_grad=True), Parameter containing:\n",
      "tensor([-3.4926e-05, -1.2271e-04,  1.9787e+00], requires_grad=True), Parameter containing:\n",
      "tensor([[ 2.9358,  5.7194, -3.6245]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3238], requires_grad=True)], 'lr': 0.1, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}]\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()  # stable version of BCE\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)  # matches your scratch trainer\n",
    "print(optimizer.param_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "08981803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.001492\n",
      "Epoch 200: Loss = 0.001413\n",
      "Epoch 400: Loss = 0.001341\n",
      "Epoch 600: Loss = 0.001275\n",
      "Epoch 800: Loss = 0.001215\n",
      "Epoch 1000: Loss = 0.001160\n",
      "Epoch 1200: Loss = 0.001110\n",
      "Epoch 1400: Loss = 0.001063\n",
      "Epoch 1600: Loss = 0.001020\n",
      "Epoch 1800: Loss = 0.000980\n",
      "Epoch 2000: Loss = 0.000943\n",
      "Epoch 2200: Loss = 0.000908\n",
      "Epoch 2400: Loss = 0.000876\n",
      "Epoch 2600: Loss = 0.000845\n",
      "Epoch 2800: Loss = 0.000817\n"
     ]
    }
   ],
   "source": [
    "epochs = 3000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(X)\n",
    "    loss = loss_fn(output, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "dc94b212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name net.0.weight : params: tensor([[ 2.2627, -1.9063],\n",
      "        [-4.0453,  4.0453],\n",
      "        [-1.9786,  2.4116]])\n",
      "name net.0.bias : params: tensor([-3.4926e-05, -1.2271e-04,  1.9787e+00])\n",
      "name net.2.weight : params: tensor([[ 2.9358,  5.7194, -3.6245]])\n",
      "name net.2.bias : params: tensor([0.3238])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"name {name} : params: {param.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "34eee601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions:\n",
      "Input: [0.0, 0.0] -> Prediction: 0.0011\n",
      "Input: [0.0, 1.0] -> Prediction: 0.9995\n",
      "Input: [1.0, 0.0] -> Prediction: 0.9991\n",
      "Input: [1.0, 1.0] -> Prediction: 0.0006\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(X)\n",
    "    preds = torch.sigmoid(logits)\n",
    "\n",
    "    print(\"\\nPredictions:\")\n",
    "    for inp, pred in zip(X, preds):\n",
    "        print(f\"Input: {inp.tolist()} -> Prediction: {pred.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
