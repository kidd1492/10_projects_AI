{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "945d4899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b6515",
   "metadata": {},
   "source": [
    "Imports:\n",
    "- torch.nn as nn [PyTorch docs :torch.nn ](https://docs.pytorch.org/docs/stable/nn.html)\n",
    "- torch.optim [PyTorch docs](https://docs.pytorch.org/docs/stable/optim.html#module-torch.optim)\n",
    "    - a package implementing various optimization algorithms.\n",
    "\n",
    "Containers :\n",
    "- nn.Module : Base class for all neural network modules.\n",
    "- nn.Sequential() : [pytorch doc :Sequential](https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential)\n",
    "\n",
    "Non-Linear Activations :\n",
    "- nn.ReLU\n",
    "- nn.Sigmoid\n",
    "\n",
    "Loss Functions :\n",
    "- nn.BCELoss() : Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities:\n",
    "- nn.BCEWithLogitsLoss() : This loss combines a Sigmoid layer and the BCELoss in one single class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09661060",
   "metadata": {},
   "source": [
    "# torch.tensor([])\n",
    "\n",
    "```\n",
    "X = torch.tensor([\n",
    "X = np.array([                      \n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "], dtype=np.float32)\n",
    "\n",
    "y = torch.tensor([\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "], dtype=np.float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "39ed41f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "X = torch.tensor([\n",
    "    [0., 0.],\n",
    "    [0., 1.],\n",
    "    [1., 0.],\n",
    "    [1., 1.]\n",
    "])\n",
    "\n",
    "y = torch.tensor([\n",
    "    [0.],\n",
    "    [1.],\n",
    "    [1.],\n",
    "    [0.]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d799166",
   "metadata": {},
   "source": [
    "Since the BCEWithLogitsLoss() has a built in sigmoid layer we can leave the sigmoid out of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "ad9a70d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XORNet_simple(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=3, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=3, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "tensor([[0.3828],\n",
      "        [0.3963],\n",
      "        [0.5611],\n",
      "        [0.5774]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class XORNet_simple(nn.Module):  #nn.Module Base class for all neural network modules\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 3),     # Input → Hidden\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3, 1),     # Hidden → Output\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "model = XORNet_simple()\n",
    "print(model)\n",
    "print()\n",
    "print(model(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "a7e32584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'params': [Parameter containing:\n",
      "tensor([[-0.3157, -0.1940],\n",
      "        [-0.4629, -0.0948],\n",
      "        [-0.5312,  0.0254]], requires_grad=True), Parameter containing:\n",
      "tensor([0.5748, 0.5537, 0.4656], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0376, -0.2596, -0.1503]], requires_grad=True), Parameter containing:\n",
      "tensor([0.5750], requires_grad=True)], 'lr': 0.1, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}]\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()  # stable version of BCE\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)  # matches your scratch trainer\n",
    "print(optimizer.param_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "08981803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.722914\n",
      "Epoch 200: Loss = 0.570865\n",
      "Epoch 400: Loss = 0.446117\n",
      "Epoch 600: Loss = 0.286559\n",
      "Epoch 800: Loss = 0.157592\n",
      "Epoch 1000: Loss = 0.094327\n",
      "Epoch 1200: Loss = 0.062523\n",
      "Epoch 1400: Loss = 0.044896\n",
      "Epoch 1600: Loss = 0.034369\n",
      "Epoch 1800: Loss = 0.027645\n",
      "Epoch 2000: Loss = 0.022946\n",
      "Epoch 2200: Loss = 0.019531\n",
      "Epoch 2400: Loss = 0.016911\n",
      "Epoch 2600: Loss = 0.014880\n",
      "Epoch 2800: Loss = 0.013243\n"
     ]
    }
   ],
   "source": [
    "epochs = 3000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(X)\n",
    "    loss = loss_fn(output, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "dc94b212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name net.0.weight : params: tensor([[-1.8757, -1.8758],\n",
      "        [-2.8742, -2.8735],\n",
      "        [-1.6994, -1.6998]])\n",
      "name net.0.bias : params: tensor([3.7531, 2.8730, 1.6992])\n",
      "name net.2.weight : params: tensor([[ 4.5178, -4.9213, -2.8574]])\n",
      "name net.2.bias : params: tensor([-3.4994])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"name {name} : params: {param.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "34eee601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions:\n",
      "Input: [0.0, 0.0] -> Prediction: 0.0039\n",
      "Input: [0.0, 1.0] -> Prediction: 0.9932\n",
      "Input: [1.0, 0.0] -> Prediction: 0.9932\n",
      "Input: [1.0, 1.0] -> Prediction: 0.0295\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(X)\n",
    "    preds = torch.sigmoid(logits)\n",
    "\n",
    "    print(\"\\nPredictions:\")\n",
    "    for inp, pred in zip(X, preds):\n",
    "        print(f\"Input: {inp.tolist()} -> Prediction: {pred.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4db9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
