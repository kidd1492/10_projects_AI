{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d01c08",
   "metadata": {},
   "source": [
    "# Project 8 : Building an RNN From Scratch (A Beginner-Friendly Walkthrough)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are designed to handle sequential data. Unlike feed‚Äëforward networks, which treat each input independently, an RNN carries information forward through time using a hidden state. This makes it suitable for time‚Äëseries data such as weather, text, audio, or stock prices.\n",
    "\n",
    "In this section, we build an RNN entirely from scratch using NumPy. The goal is to understand the mechanics of recurrence, hidden state updates, and backpropagation through time (BPTT) without relying on deep‚Äëlearning frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a135edf7",
   "metadata": {},
   "source": [
    "# Generating a Synthetic Weather Time-Series\n",
    "\n",
    "To train an RNN, we need a sequence with memory. Weather is a natural example: \n",
    "today‚Äôs temperature depends on previous days plus some randomness.\n",
    "\n",
    "We generate a simple autoregressive process:\n",
    "\n",
    "`T_t = 0.7 * T_(t-1) + 0.2 * T_(t-2) + Normal(0, 0.5)`\n",
    "\n",
    "This produces a smooth temperature sequence with short-term memory and noise.\n",
    "We will use the first 5 days to predict day 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f272a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = []\n",
    "temps.append(20)   # seed day 0\n",
    "temps.append(21)   # seed day 1\n",
    "\n",
    "for t in range(2, 100):\n",
    "    next_temp = (\n",
    "        0.7 * temps[-1] +\n",
    "        0.2 * temps[-2] +\n",
    "        np.random.normal(0, 0.5)\n",
    "    )\n",
    "    temps.append(next_temp)\n",
    "\n",
    "input_sq = temps[:5]\n",
    "target = temps[5]\n",
    "print(input_sq, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432c8fb1",
   "metadata": {},
   "source": [
    "## Initialize the RNN Parameters\n",
    "\n",
    "**A minimal RNN cell with:**\n",
    "\n",
    "- W_xh: input ‚Üí hidden\n",
    "- W_hh: hidden ‚Üí hidden (the recurrence)\n",
    "- b_h: hidden bias\n",
    "\n",
    "\n",
    "- W_hy: hidden ‚Üí output\n",
    "- b_y: output bias\n",
    "\n",
    "These are all randomly initialized, just like in any other neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f86fd291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights:\n",
      "W_xh: [-0.0153758]\n",
      "W_hh: [-0.01113899]\n",
      "b_h : [-0.65919943] \n",
      "\n",
      "W_hy: [0.01334814]\n",
      "b_y : 1.4508480445114487\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 1\n",
    "\n",
    "W_xh = np.random.randn(hidden_size) * 0.01     # input ‚Üí hidden\n",
    "W_hh = np.random.randn(hidden_size) * 0.01    # hidden ‚Üí hidden\n",
    "b_h  = np.random.randn(hidden_size)    # hidden bias\n",
    "\n",
    "W_hy = np.random.randn(hidden_size) * 0.01      # hidden ‚Üí output\n",
    "b_y  = np.random.randn()                 # output bias\n",
    "\n",
    "print(\"\\nWeights:\")\n",
    "print(\"W_xh:\", W_xh)\n",
    "print(\"W_hh:\", W_hh)\n",
    "print(\"b_h :\", b_h, \"\\n\")\n",
    "print(\"W_hy:\", W_hy)\n",
    "print(\"b_y :\", b_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d485f",
   "metadata": {},
   "source": [
    "## Forward Pass Through Time\n",
    "\n",
    "**This is where the RNN differs from a normal feed‚Äëforward network.**\n",
    "\n",
    "For each timestep in the input sequence:\n",
    "\n",
    "Compute the pre‚Äëactivation\n",
    "\n",
    "- **ùëé_ùë° = (W_xh * x_t) + (W_hh * h_t[-1]) + b_h**\n",
    "\n",
    "Apply the activation\n",
    "\n",
    "- **h_t = tanh(a_t)**\n",
    "\n",
    "Store:\n",
    "\n",
    "- the raw activation (a_t)\n",
    "- the hidden state (h_t)\n",
    "\n",
    "**This loop is the ‚Äúunrolling through time‚Äù that gives RNNs memory.**\n",
    "\n",
    "**After the final timestep, compute the output:**\n",
    "\n",
    "- **y = W_hy * h_t + b_y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd70a3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final h: [-0.71339194]\n",
      "Prediction: [1.44132559]\n",
      "Target: -0.35826484905289013\n"
     ]
    }
   ],
   "source": [
    "# Forward pass with storage for BPTT\n",
    "hs = [0.0]   # h_0\n",
    "raws = []    # a_t\n",
    "\n",
    "h = 0.0\n",
    "for x in input_sq:\n",
    "    a = W_xh * x + W_hh * h + b_h\n",
    "    h = np.tanh(a)\n",
    "    raws.append(a)\n",
    "    hs.append(h)\n",
    "\n",
    "y_pred = W_hy * h + b_y\n",
    "\n",
    "print(\"Final h:\", h)\n",
    "print(\"Prediction:\", y_pred)\n",
    "print(\"Target:\", target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efbd505",
   "metadata": {},
   "source": [
    "## Compute the Loss\n",
    "**mean squared error:**\n",
    "\n",
    "- **L = (y_pred - target)2**\n",
    "\n",
    "This measures how far the prediction is from the true next temperature.\n",
    "\n",
    "\n",
    "## Backpropagation Through Time (BPTT)\n",
    "**This is the heart of training an RNN.**\n",
    "\n",
    "**Step A : Start at the output**\n",
    "\n",
    "Compute:\n",
    "\n",
    "- gradient of the loss wrt the output\n",
    "- gradient wrt W_hy and b_y\n",
    "\n",
    "**gradient flowing back into the final hidden state**\n",
    "\n",
    "**Step B : Walk backward through each timestep**\n",
    "\n",
    "For each timestep (in reverse):\n",
    "\n",
    "Compute derivative of tanh\n",
    "\n",
    "‚àÇ‚Ñéùë° / ‚àÇùëéùë° = 1 - tanh**2(a_t)\n",
    "\n",
    "Compute gradients for:\n",
    "\n",
    "- W_xh\n",
    "- W_hh\n",
    "- b_h\n",
    "\n",
    "Propagate gradient to the previous hidden state using W_hh\n",
    "\n",
    "This is the ‚Äúthrough time‚Äù part ‚Äî the gradient flows backward across all timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222172a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "loss = (y_pred - target)**2\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "# dL/dy\n",
    "dL_dy = 2 * (y_pred - target)\n",
    "\n",
    "# Output layer gradients\n",
    "dW_hy = dL_dy * hs[-1]\n",
    "db_y  = dL_dy\n",
    "\n",
    "# Gradient flowing into last hidden state\n",
    "dh_next = dL_dy * W_hy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0261f4",
   "metadata": {},
   "source": [
    "## Update the Parameters\n",
    "**Just like any neural network:**\n",
    "\n",
    "ùúÉ‚Üê ùúÉ ‚àí ùúÇ ‚ãÖ ‚àá ùúÉ\n",
    "You applied this to all weights and biases:\n",
    "\n",
    "- W_xh, W_hh, b_h\n",
    "- W_hy, b_y\n",
    "\n",
    "**This is standard gradient descent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182d885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RNN parameter grads\n",
    "dW_xh = 0.0\n",
    "dW_hh = 0.0\n",
    "db_h  = 0.0\n",
    "\n",
    "# Backprop through time\n",
    "for t in reversed(range(len(input_sq))):\n",
    "    a_t = raws[t]\n",
    "    h_prev = hs[t]\n",
    "    x_t = input_sq[t]\n",
    "\n",
    "    # derivative of tanh\n",
    "    da = (1 - np.tanh(a_t)**2) * dh_next\n",
    "\n",
    "    # accumulate grads\n",
    "    dW_xh += da * x_t\n",
    "    dW_hh += da * h_prev\n",
    "    db_h  += da\n",
    "\n",
    "    # propagate to previous h\n",
    "    dh_next = da * W_hh\n",
    "\n",
    "print(\"dW_xh:\", dW_xh)\n",
    "print(\"dW_hh:\", dW_hh)\n",
    "print(\"db_h :\", db_h)\n",
    "print(\"dW_hy:\", dW_hy)\n",
    "print(\"db_y :\", db_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3c17e6",
   "metadata": {},
   "source": [
    "## Wrap Everything Into a Class\n",
    "You encapsulated the logic into:\n",
    "\n",
    "RNNCell\n",
    "Handles one timestep\n",
    "\n",
    "Computes hidden state\n",
    "\n",
    "RNNPredictor\n",
    "Unrolls the RNN across a sequence\n",
    "\n",
    "Computes the output\n",
    "\n",
    "Stores activations\n",
    "\n",
    "Performs BPTT\n",
    "\n",
    "Updates parameters\n",
    "\n",
    "This mirrors the structure of real deep‚Äëlearning libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28fbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.b_h  = np.random.randn(hidden_size)\n",
    "\n",
    "    def forward(self, x_t, h_prev):\n",
    "        raw = self.W_xh @ x_t + self.W_hh @ h_prev + self.b_h\n",
    "        h_t = np.tanh(raw)\n",
    "        return h_t, raw\n",
    "\n",
    "\n",
    "class RNNPredictor:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.cell = RNNCell(input_size, hidden_size)\n",
    "\n",
    "        self.W_hy = np.random.randn(1, hidden_size) * 0.01\n",
    "        self.b_y  = np.random.randn()\n",
    "\n",
    "    def forward_sequence(self, sequence):\n",
    "        h = np.zeros(self.cell.hidden_size)\n",
    "        hs = [h]      # store hidden states\n",
    "        raws = []     # store raw pre-activations\n",
    "\n",
    "        for x in sequence:\n",
    "            x_t = np.array([x])\n",
    "            h, raw = self.cell.forward(x_t, h)\n",
    "            hs.append(h)\n",
    "            raws.append(raw)\n",
    "\n",
    "        y_pred = self.W_hy @ h + self.b_y\n",
    "        return y_pred, hs, raws\n",
    "\n",
    "    def train_step(self, sequence, target, lr=0.0001):\n",
    "        y_pred, hs, raws = self.forward_sequence(sequence)\n",
    "\n",
    "        # ----- Loss -----\n",
    "        loss = (y_pred - target)**2\n",
    "\n",
    "        # ----- Gradients -----\n",
    "        dL_dy = 2 * (y_pred - target)  # scalar\n",
    "\n",
    "        # Output layer grads\n",
    "        dW_hy = dL_dy * hs[-1].reshape(1, -1)\n",
    "        db_y  = dL_dy\n",
    "\n",
    "        # Backprop into last hidden state\n",
    "        dh_next = (self.W_hy.T * dL_dy).flatten()\n",
    "\n",
    "        # Initialize grads for RNN cell\n",
    "        dW_xh = np.zeros_like(self.cell.W_xh)\n",
    "        dW_hh = np.zeros_like(self.cell.W_hh)\n",
    "        db_h  = np.zeros_like(self.cell.b_h)\n",
    "\n",
    " \n",
    "\n",
    "        # ----- BPTT -----\n",
    "        for t in reversed(range(len(sequence))):\n",
    "            raw = raws[t]\n",
    "            h_prev = hs[t]\n",
    "        \n",
    "            dtanh = (1 - np.tanh(raw)**2) * dh_next\n",
    "        \n",
    "            x_t = np.array([sequence[t]])\n",
    "            dW_xh += dtanh.reshape(-1,1) @ x_t.reshape(1,-1)\n",
    "            dW_hh += dtanh.reshape(-1,1) @ h_prev.reshape(1,-1)\n",
    "            db_h  += dtanh\n",
    "        \n",
    "            dh_next = self.cell.W_hh.T @ dtanh\n",
    "        \n",
    "        # ----- Gradient Clipping -----\n",
    "        clip_value = 1.0\n",
    "        dW_xh = np.clip(dW_xh, -clip_value, clip_value)\n",
    "        dW_hh = np.clip(dW_hh, -clip_value, clip_value)\n",
    "        db_h  = np.clip(db_h,  -clip_value, clip_value)\n",
    "        dW_hy = np.clip(dW_hy, -clip_value, clip_value)\n",
    "        db_y  = np.clip(db_y,  -clip_value, clip_value)\n",
    "        \n",
    "        # ----- Update weights -----\n",
    "        self.W_hy -= lr * dW_hy\n",
    "        self.b_y  -= lr * db_y\n",
    "        self.cell.W_xh -= lr * dW_xh\n",
    "        self.cell.W_hh -= lr * dW_hh\n",
    "        self.cell.b_h  -= lr * db_h\n",
    "\n",
    "\n",
    "        return loss, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9044ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(temps, seq_len=5):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(temps) - seq_len):\n",
    "        X.append(temps[i:i+seq_len])\n",
    "        y.append(temps[i+seq_len])\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9830c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_dataset(temps, seq_len=5)\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22256ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNPredictor(input_size=1, hidden_size=20)\n",
    "\n",
    "for epoch in range(2500):\n",
    "    total_loss = 0\n",
    "\n",
    "    for seq, target in zip(X, y):\n",
    "        loss, pred = model.train_step(seq, target)\n",
    "        total_loss += loss\n",
    "\n",
    "    if epoch % 250 == 0:\n",
    "        print(f\"epoch {epoch}, total_loss={total_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa9370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = temps[:5]          # or any 5‚Äëday window\n",
    "pred, hs, raws = model.forward_sequence(test_seq)\n",
    "\n",
    "print(\"Input:\", test_seq)\n",
    "print(\"Prediction:\", pred)\n",
    "print(\"True next value:\", temps[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0628337",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = (pred - temps[5])\n",
    "error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
