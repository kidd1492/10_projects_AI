{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d01c08",
   "metadata": {},
   "source": [
    "# Project 8 : RNN\n",
    "\n",
    "1. Generate the Time‚ÄëSeries Data\n",
    "Created a synthetic weather sequence using an autoregressive rule:\n",
    "\n",
    "Each new temperature depends on the previous two temperatures\n",
    "- Plus a bit of Gaussian noise\n",
    "\n",
    "This gives a realistic time‚Äëseries pattern with memory and randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0593c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f272a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 21, 19.165781784734058, 16.087429935013184, 15.307597421130193] 14.672780156181451\n"
     ]
    }
   ],
   "source": [
    "temps = []\n",
    "temps.append(20)   # seed day 0\n",
    "temps.append(21)   # seed day 1\n",
    "\n",
    "for t in range(2, 100):\n",
    "    next_temp = (\n",
    "        0.7 * temps[-1] +\n",
    "        0.2 * temps[-2] +\n",
    "        np.random.normal(0, 0.5)\n",
    "    )\n",
    "    temps.append(next_temp)\n",
    "\n",
    "input_sq = temps[:5]\n",
    "target = temps[5]\n",
    "print(input_sq, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432c8fb1",
   "metadata": {},
   "source": [
    "## Initialize the RNN Parameters\n",
    "\n",
    "**A minimal RNN cell with:**\n",
    "\n",
    "- W_xh: input ‚Üí hidden\n",
    "- W_hh: hidden ‚Üí hidden (the recurrence)\n",
    "- b_h: hidden bias\n",
    "\n",
    "\n",
    "- W_hy: hidden ‚Üí output\n",
    "- b_y: output bias\n",
    "\n",
    "These are all randomly initialized, just like in any other neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f86fd291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights:\n",
      "W_xh: [-0.01120693]\n",
      "W_hh: [0.00323926]\n",
      "b_h : [1.49454101] \n",
      "\n",
      "W_hy: [0.0246548]\n",
      "b_y : -1.1758237039803698\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 1\n",
    "\n",
    "W_xh = np.random.randn(hidden_size) * 0.01     # input ‚Üí hidden\n",
    "W_hh = np.random.randn(hidden_size) * 0.01    # hidden ‚Üí hidden\n",
    "b_h  = np.random.randn(hidden_size)    # hidden bias\n",
    "\n",
    "W_hy = np.random.randn(hidden_size) * 0.01      # hidden ‚Üí output\n",
    "b_y  = np.random.randn()                 # output bias\n",
    "\n",
    "print(\"\\nWeights:\")\n",
    "print(\"W_xh:\", W_xh)\n",
    "print(\"W_hh:\", W_hh)\n",
    "print(\"b_h :\", b_h, \"\\n\")\n",
    "print(\"W_hy:\", W_hy)\n",
    "print(\"b_y :\", b_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d485f",
   "metadata": {},
   "source": [
    "## Forward Pass Through Time\n",
    "\n",
    "**This is where the RNN differs from a normal feed‚Äëforward network.**\n",
    "\n",
    "For each timestep in the input sequence:\n",
    "\n",
    "Compute the pre‚Äëactivation\n",
    "\n",
    "- **ùëé_ùë° = (W_xh * x_t) + (W_hh * h_t[-1]) + b_h**\n",
    "\n",
    "Apply the activation\n",
    "\n",
    "- **h_t = tanh(a_t)**\n",
    "\n",
    "Store:\n",
    "\n",
    "- the raw activation (a_t)\n",
    "- the hidden state (h_t)\n",
    "\n",
    "**This loop is the ‚Äúunrolling through time‚Äù that gives RNNs memory.**\n",
    "\n",
    "**After the final timestep, compute the output:**\n",
    "\n",
    "- **y = W_hy * h_t + b_y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd70a3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final h: [0.86821787]\n",
      "Prediction: [-1.15441796]\n",
      "Target: 14.672780156181451\n"
     ]
    }
   ],
   "source": [
    "# Forward pass with storage for BPTT\n",
    "hs = [0.0]   # h_0\n",
    "raws = []    # a_t\n",
    "\n",
    "h = 0.0\n",
    "for x in input_sq:\n",
    "    a = W_xh * x + W_hh * h + b_h\n",
    "    h = np.tanh(a)\n",
    "    raws.append(a)\n",
    "    hs.append(h)\n",
    "\n",
    "y_pred = W_hy * h + b_y\n",
    "\n",
    "print(\"Final h:\", h)\n",
    "print(\"Prediction:\", y_pred)\n",
    "print(\"Target:\", target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efbd505",
   "metadata": {},
   "source": [
    "## Compute the Loss\n",
    "**mean squared error:**\n",
    "\n",
    "- **L = (y_pred - target)2**\n",
    "\n",
    "This measures how far the prediction is from the true next temperature.\n",
    "\n",
    "\n",
    "## Backpropagation Through Time (BPTT)\n",
    "**This is the heart of training an RNN.**\n",
    "\n",
    "**Step A : Start at the output**\n",
    "\n",
    "Compute:\n",
    "\n",
    "- gradient of the loss wrt the output\n",
    "- gradient wrt W_hy and b_y\n",
    "\n",
    "**gradient flowing back into the final hidden state**\n",
    "\n",
    "**Step B : Walk backward through each timestep**\n",
    "\n",
    "For each timestep (in reverse):\n",
    "\n",
    "Compute derivative of tanh\n",
    "\n",
    "‚àÇ‚Ñéùë° / ‚àÇùëéùë° = 1 - tanh**2(a_t)\n",
    "\n",
    "Compute gradients for:\n",
    "\n",
    "- W_xh\n",
    "- W_hh\n",
    "- b_h\n",
    "\n",
    "Propagate gradient to the previous hidden state using W_hh\n",
    "\n",
    "This is the ‚Äúthrough time‚Äù part ‚Äî the gradient flows backward across all timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "222172a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: [250.50020029]\n"
     ]
    }
   ],
   "source": [
    "# Loss\n",
    "loss = (y_pred - target)**2\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "# dL/dy\n",
    "dL_dy = 2 * (y_pred - target)\n",
    "\n",
    "# Output layer gradients\n",
    "dW_hy = dL_dy * hs[-1]\n",
    "db_y  = dL_dy\n",
    "\n",
    "# Gradient flowing into last hidden state\n",
    "dh_next = dL_dy * W_hy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0261f4",
   "metadata": {},
   "source": [
    "## Update the Parameters\n",
    "**Just like any neural network:**\n",
    "\n",
    "ùúÉ‚Üê ùúÉ ‚àí ùúÇ ‚ãÖ ‚àá ùúÉ\n",
    "You applied this to all weights and biases:\n",
    "\n",
    "- W_xh, W_hh, b_h\n",
    "- W_hy, b_y\n",
    "\n",
    "**This is standard gradient descent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "182d885e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW_xh: [-2.94371973]\n",
      "dW_hh: [-0.16653561]\n",
      "db_h : [-0.19229653]\n",
      "dW_hy: [-27.48291246]\n",
      "db_y : [-31.65439624]\n"
     ]
    }
   ],
   "source": [
    "# Initialize RNN parameter grads\n",
    "dW_xh = 0.0\n",
    "dW_hh = 0.0\n",
    "db_h  = 0.0\n",
    "\n",
    "# Backprop through time\n",
    "for t in reversed(range(len(input_sq))):\n",
    "    a_t = raws[t]\n",
    "    h_prev = hs[t]\n",
    "    x_t = input_sq[t]\n",
    "\n",
    "    # derivative of tanh\n",
    "    da = (1 - np.tanh(a_t)**2) * dh_next\n",
    "\n",
    "    # accumulate grads\n",
    "    dW_xh += da * x_t\n",
    "    dW_hh += da * h_prev\n",
    "    db_h  += da\n",
    "\n",
    "    # propagate to previous h\n",
    "    dh_next = da * W_hh\n",
    "\n",
    "print(\"dW_xh:\", dW_xh)\n",
    "print(\"dW_hh:\", dW_hh)\n",
    "print(\"db_h :\", db_h)\n",
    "print(\"dW_hy:\", dW_hy)\n",
    "print(\"db_y :\", db_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3c17e6",
   "metadata": {},
   "source": [
    "## Wrap Everything Into a Class\n",
    "You encapsulated the logic into:\n",
    "\n",
    "RNNCell\n",
    "Handles one timestep\n",
    "\n",
    "Computes hidden state\n",
    "\n",
    "RNNPredictor\n",
    "Unrolls the RNN across a sequence\n",
    "\n",
    "Computes the output\n",
    "\n",
    "Stores activations\n",
    "\n",
    "Performs BPTT\n",
    "\n",
    "Updates parameters\n",
    "\n",
    "This mirrors the structure of real deep‚Äëlearning libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f28fbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.b_h  = np.random.randn(hidden_size)\n",
    "\n",
    "    def forward(self, x_t, h_prev):\n",
    "        raw = self.W_xh @ x_t + self.W_hh @ h_prev + self.b_h\n",
    "        h_t = np.tanh(raw)\n",
    "        return h_t, raw\n",
    "\n",
    "\n",
    "class RNNPredictor:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.cell = RNNCell(input_size, hidden_size)\n",
    "\n",
    "        self.W_hy = np.random.randn(1, hidden_size) * 0.01\n",
    "        self.b_y  = np.random.randn()\n",
    "\n",
    "    def forward_sequence(self, sequence):\n",
    "        h = np.zeros(self.cell.hidden_size)\n",
    "        hs = [h]      # store hidden states\n",
    "        raws = []     # store raw pre-activations\n",
    "\n",
    "        for x in sequence:\n",
    "            x_t = np.array([x])\n",
    "            h, raw = self.cell.forward(x_t, h)\n",
    "            hs.append(h)\n",
    "            raws.append(raw)\n",
    "\n",
    "        y_pred = self.W_hy @ h + self.b_y\n",
    "        return y_pred, hs, raws\n",
    "\n",
    "    def train_step(self, sequence, target, lr=0.0001):\n",
    "        y_pred, hs, raws = self.forward_sequence(sequence)\n",
    "\n",
    "        # ----- Loss -----\n",
    "        loss = (y_pred - target)**2\n",
    "\n",
    "        # ----- Gradients -----\n",
    "        dL_dy = 2 * (y_pred - target)  # scalar\n",
    "\n",
    "        # Output layer grads\n",
    "        dW_hy = dL_dy * hs[-1].reshape(1, -1)\n",
    "        db_y  = dL_dy\n",
    "\n",
    "        # Backprop into last hidden state\n",
    "        dh_next = (self.W_hy.T * dL_dy).flatten()\n",
    "\n",
    "        # Initialize grads for RNN cell\n",
    "        dW_xh = np.zeros_like(self.cell.W_xh)\n",
    "        dW_hh = np.zeros_like(self.cell.W_hh)\n",
    "        db_h  = np.zeros_like(self.cell.b_h)\n",
    "\n",
    " \n",
    "\n",
    "        # ----- BPTT -----\n",
    "        for t in reversed(range(len(sequence))):\n",
    "            raw = raws[t]\n",
    "            h_prev = hs[t]\n",
    "        \n",
    "            dtanh = (1 - np.tanh(raw)**2) * dh_next\n",
    "        \n",
    "            x_t = np.array([sequence[t]])\n",
    "            dW_xh += dtanh.reshape(-1,1) @ x_t.reshape(1,-1)\n",
    "            dW_hh += dtanh.reshape(-1,1) @ h_prev.reshape(1,-1)\n",
    "            db_h  += dtanh\n",
    "        \n",
    "            dh_next = self.cell.W_hh.T @ dtanh\n",
    "        \n",
    "        # ----- Gradient Clipping -----\n",
    "        clip_value = 1.0\n",
    "        dW_xh = np.clip(dW_xh, -clip_value, clip_value)\n",
    "        dW_hh = np.clip(dW_hh, -clip_value, clip_value)\n",
    "        db_h  = np.clip(db_h,  -clip_value, clip_value)\n",
    "        dW_hy = np.clip(dW_hy, -clip_value, clip_value)\n",
    "        db_y  = np.clip(db_y,  -clip_value, clip_value)\n",
    "        \n",
    "        # ----- Update weights -----\n",
    "        self.W_hy -= lr * dW_hy\n",
    "        self.b_y  -= lr * db_y\n",
    "        self.cell.W_xh -= lr * dW_xh\n",
    "        self.cell.W_hh -= lr * dW_hh\n",
    "        self.cell.b_h  -= lr * db_h\n",
    "\n",
    "\n",
    "        return loss, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9044ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(temps, seq_len=5):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(temps) - seq_len):\n",
    "        X.append(temps[i:i+seq_len])\n",
    "        y.append(temps[i+seq_len])\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9830c1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95, 5) (95,)\n"
     ]
    }
   ],
   "source": [
    "X, y = make_dataset(temps, seq_len=5)\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f22256ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, total_loss=[2141.77044345]\n",
      "epoch 250, total_loss=[565.42410998]\n",
      "epoch 500, total_loss=[167.68090605]\n",
      "epoch 750, total_loss=[85.33345951]\n",
      "epoch 1000, total_loss=[53.24991208]\n",
      "epoch 1250, total_loss=[38.46009129]\n",
      "epoch 1500, total_loss=[31.93027038]\n",
      "epoch 1750, total_loss=[27.95588329]\n",
      "epoch 2000, total_loss=[26.03146945]\n",
      "epoch 2250, total_loss=[25.23165896]\n"
     ]
    }
   ],
   "source": [
    "model = RNNPredictor(input_size=1, hidden_size=20)\n",
    "\n",
    "for epoch in range(2500):\n",
    "    total_loss = 0\n",
    "\n",
    "    for seq, target in zip(X, y):\n",
    "        loss, pred = model.train_step(seq, target)\n",
    "        total_loss += loss\n",
    "\n",
    "    if epoch % 250 == 0:\n",
    "        print(f\"epoch {epoch}, total_loss={total_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efa9370d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [20, 21, 19.165781784734058, 16.087429935013184, 15.307597421130193]\n",
      "Prediction: [12.81218303]\n",
      "True next value: 14.672780156181451\n"
     ]
    }
   ],
   "source": [
    "test_seq = temps[:5]          # or any 5‚Äëday window\n",
    "pred, hs, raws = model.forward_sequence(test_seq)\n",
    "\n",
    "print(\"Input:\", test_seq)\n",
    "print(\"Prediction:\", pred)\n",
    "print(\"True next value:\", temps[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0628337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.86059713])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = (pred - temps[5])\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728a698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
