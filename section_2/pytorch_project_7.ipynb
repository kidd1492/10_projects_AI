{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc29d24",
   "metadata": {},
   "source": [
    "# Project 7 : Introduction to PyTorch\n",
    "**Tensors, Autograd, and Rebuilding the Project 6 Network**\n",
    "\n",
    "In Project 6, we built a neural network framework completely from scratch.\n",
    "\n",
    " We implemented:\n",
    "\n",
    "- Dense layers\n",
    "- Activation functions\n",
    "- Forward and backward passes\n",
    "- A training loop\n",
    "- Gradient updates\n",
    "- Model saving and loading\n",
    "\n",
    "By the end, we had a tiny version of PyTorch or TensorFlow like framework that worked exactly like a real deep‑learning library.\n",
    "\n",
    "**Project 7 is where we switch from building the tools… to using the tools.**\n",
    "\n",
    "\n",
    "# How PyTorch Maps to Project 6\n",
    "- Everything you built manually now has a PyTorch equivalent:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "945d4899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b6515",
   "metadata": {},
   "source": [
    "Imports:\n",
    "- torch.nn as nn [PyTorch docs :torch.nn ](https://docs.pytorch.org/docs/stable/nn.html)\n",
    "- torch.optim [PyTorch docs](https://docs.pytorch.org/docs/stable/optim.html#module-torch.optim)\n",
    "    - a package implementing various optimization algorithms.\n",
    "\n",
    "Containers :\n",
    "- nn.Module : Base class for all neural network modules.\n",
    "- nn.Sequential() : [pytorch doc :Sequential](https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential)\n",
    "\n",
    "Non-Linear Activations :\n",
    "- nn.ReLU\n",
    "- nn.Sigmoid\n",
    "\n",
    "Loss Functions :\n",
    "- nn.BCELoss() : Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities:\n",
    "- nn.BCEWithLogitsLoss() : This loss combines a Sigmoid layer and the BCELoss in one single class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09661060",
   "metadata": {},
   "source": [
    "# What Is a Tensor?\n",
    "- A tensor is PyTorch’s fundamental data structure.\n",
    "- It looks like a NumPy array, but with two superpowers:\n",
    "\n",
    "1. Tensors can run on a GPU\n",
    "- This allows PyTorch to scale from XOR → CNNs → Transformers without changing your code.\n",
    "\n",
    "2. Tensors track operations for autograd\n",
    "- If you set:\n",
    "- x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "\n",
    "**PyTorch builds a computation graph behind the scenes.**\n",
    "\n",
    "Every operation is recorded so PyTorch can compute gradients automatically during backprop.\n",
    "\n",
    "**This is the key difference:**\n",
    "\n",
    "- NumPy array: just numbers\n",
    "- PyTorch tensor: numbers + history of operations\n",
    "\n",
    "This is why PyTorch can compute derivatives without us writing a single gradient formula.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39ed41f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "X = torch.tensor([\n",
    "    [0., 0.],\n",
    "    [0., 1.],\n",
    "    [1., 0.],\n",
    "    [1., 1.]\n",
    "])\n",
    "\n",
    "y = torch.tensor([\n",
    "    [0.],\n",
    "    [1.],\n",
    "    [1.],\n",
    "    [0.]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d799166",
   "metadata": {},
   "source": [
    "1. set up a class with nn.Module\n",
    "2. Initialize the class\n",
    "3. initialize the parent class. \n",
    "\n",
    "4. Define the network\n",
    "self.name_of_network = nn.Sequential()\n",
    "- Sequential takes the layers and activation separately\n",
    "- The layer nn.Linear() takes the input and output dimensions.\n",
    "- Followed by an activation function. \n",
    "\n",
    "Since the BCEWithLogitsLoss() has a built-in sigmoid layer we can leave the sigmoid out of the model.\n",
    "\n",
    "5. Define a method for forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad9a70d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XORNet_simple(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=4, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=4, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class XORNet_simple(nn.Module):  #nn.Module Base class for all neural network modules\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 4),     # Input → Hidden\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1),     # Hidden → Output\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "model = XORNet_simple()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5873352",
   "metadata": {},
   "source": [
    "## What happens when you use BCEWithLogitsLoss()\n",
    "\n",
    "**BCEWithLogitsLoss does two things in one:**\n",
    "\n",
    "1. Applies the sigmoid activation:\n",
    "    - σ(z2)=11+e−z2\n",
    "2. Computes binary cross‑entropy:\n",
    "    - loss=−[ylog⁡(σ(z2))+(1−y)log⁡(1−σ(z2))]\n",
    "\n",
    "**So when you do:**\n",
    "\n",
    "python\n",
    "**loss_fn = nn.BCEWithLogitsLoss()**\n",
    "loss = loss_fn(model(X), y)\n",
    "\n",
    "**PyTorch internally performs:**\n",
    "sigmoid on your raw outputs\n",
    "then BCE\n",
    "\n",
    "### You do not need to put a Sigmoid() in your model.\n",
    "Adding the sigmoid manually would be incorrect because you’d be applying sigmoid twice; this is exactly what I did when I first set up this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7e32584",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()  # stable version of BCE\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)  # matches your scratch trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba404c",
   "metadata": {},
   "source": [
    "## The learning Loop\n",
    "\n",
    "just like all the other project we have:\n",
    "- intilized weights\n",
    "- defined the model with a forward pass\n",
    "- defined the loss function (BCEWithLogitsLoss())\n",
    "- defined the update rule (optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08981803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.730838\n",
      "Epoch 200: Loss = 0.614836\n",
      "Epoch 400: Loss = 0.427541\n",
      "Epoch 600: Loss = 0.208978\n",
      "Epoch 800: Loss = 0.097585\n",
      "Epoch 1000: Loss = 0.053814\n",
      "Epoch 1200: Loss = 0.034265\n",
      "Epoch 1400: Loss = 0.024029\n",
      "Epoch 1600: Loss = 0.018137\n",
      "Epoch 1800: Loss = 0.014326\n",
      "Epoch 2000: Loss = 0.011692\n",
      "Epoch 2200: Loss = 0.009812\n",
      "Epoch 2400: Loss = 0.008402\n",
      "Epoch 2600: Loss = 0.007324\n",
      "Epoch 2800: Loss = 0.006466\n"
     ]
    }
   ],
   "source": [
    "epochs = 3000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(X) #this is last hidden layers output\n",
    "    loss = loss_fn(output, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc94b212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name net.0.weight : params: tensor([[ 0.3264, -0.4394],\n",
      "        [-0.9206,  0.0206],\n",
      "        [ 3.4775,  3.4777],\n",
      "        [ 2.3426,  2.7847]])\n",
      "name net.0.bias : params: tensor([-3.3494e-01,  1.7041e+00, -3.4781e+00,  3.2628e-04])\n",
      "name net.2.weight : params: tensor([[-0.4563, -1.7206, -6.0009,  3.5800]])\n",
      "name net.2.bias : params: tensor([-1.7278])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"name {name} : params: {param.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19336cdd",
   "metadata": {},
   "source": [
    "# What the Model Actually Returns: Understanding Logits\n",
    "\n",
    "This is the dot product plus bias before any activation function.\n",
    "\n",
    "- Logits can be any real number. \n",
    "-  They are not between 0 and 1. \n",
    "- They are not yet interpretable as probabilities.\n",
    "\n",
    "This is intentional. PyTorch wants logits because they are numerically stable for training.\n",
    "\n",
    "## Making Predictions\n",
    "During inference, we apply sigmoid manually to convert logits into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34eee601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions:\n",
      "Input: [0.0, 0.0] -> Prediction: 0.0094\n",
      "Input: [0.0, 1.0] -> Prediction: 0.9949\n",
      "Input: [1.0, 0.0] -> Prediction: 0.9951\n",
      "Input: [1.0, 1.0] -> Prediction: 0.0036\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(X)\n",
    "    preds = torch.sigmoid(logits)\n",
    "\n",
    "    print(\"\\nPredictions:\")\n",
    "    for inp, pred in zip(X, preds):\n",
    "        print(f\"Input: {inp.tolist()} -> Prediction: {pred.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
