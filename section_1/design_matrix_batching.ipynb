{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03339a1f",
   "metadata": {},
   "source": [
    "# Project 5 â€” The Design Matrix & The Structure of Neural Networks\n",
    "\n",
    "**Why This Project Exists**\n",
    "Projects 1â€“4 taught you the operations of machine learning:\n",
    "\n",
    "- dot products\n",
    "- gradients\n",
    "- MSE\n",
    "- crossâ€‘entropy\n",
    "- logistic regression\n",
    "- hidden layers\n",
    "\n",
    "But they didnâ€™t yet reveal the structural object that unifies all of them:\n",
    "\n",
    "### The Design Matrix **ğ‘‹**\n",
    "\n",
    "This project shows:\n",
    "\n",
    "- Why ML always uses a matrix\n",
    "- how neural networks generalize it\n",
    "- how this simplifies the system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65d2168",
   "metadata": {},
   "source": [
    "## What Is the Design Matrix?\n",
    "\n",
    "**A design matrix is how machine learning represents data.**\n",
    "\n",
    "Each row = one example = \n",
    "- n = number of examples (rows of X)\n",
    "\n",
    "Each column = one feature\n",
    "- d = number of features (columns of X)\n",
    "\n",
    "So when we say:\n",
    "\n",
    "ğ‘‹\n",
    "âˆˆ\n",
    "ğ‘…\n",
    "ğ‘›\n",
    "Ã—\n",
    "ğ‘‘\n",
    "\n",
    "We mean:\n",
    "\n",
    "- X has n rows (one per data point)\n",
    "- X has d columns (one per feature)\n",
    "\n",
    "![image](mathematics/matrix_shape.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e87d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 2.],\n",
       "        [3., 4.],\n",
       "        [5., 6.]]),\n",
       " (3, 2))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 3 samples, 2 features\n",
    "X = np.array([\n",
    "    [1.0, 2.0],\n",
    "    [3.0, 4.0],\n",
    "    [5.0, 6.0]\n",
    "])\n",
    "\n",
    "X, X.shape\n",
    "\n",
    "# this is the simplest possible design matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d652a",
   "metadata": {},
   "source": [
    "## From One Example to Many: Matrix Multiplication\n",
    "\n",
    "Now suppose we want predictions for all n examples at once.\n",
    "\n",
    "Instead of computing:\n",
    "\n",
    "y^(1),y^(2),Y^(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fdd84a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2 = 3.0, 4.0\n",
    "w1, w2 = 2.0, -1.0\n",
    "b = 0.5\n",
    "\n",
    "y_hat_scalar = w1*x1 + w2*x2 + b\n",
    "y_hat_scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e0e0d9",
   "metadata": {},
   "source": [
    "## Vector Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cfee50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([3.0, 4.0])\n",
    "w = np.array([2.0, -1.0])\n",
    "b = 0.5\n",
    "\n",
    "y_hat_vector = w @ x + b\n",
    "y_hat_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d3e67f",
   "metadata": {},
   "source": [
    "Matrix Form (All n Examples at Once)\n",
    "Stack all examples into the design matrix:\n",
    "\n",
    "ğ‘¦^ = ğ‘‹ğ‘¤ + ğ‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b61f3e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5],\n",
       "       [2.5],\n",
       "       [4.5]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [1.0, 2.0],\n",
    "    [3.0, 4.0],\n",
    "    [5.0, 6.0]\n",
    "])\n",
    "\n",
    "w = np.array([[2.0], [-1.0]])  # shape (d,1)\n",
    "b = 0.5\n",
    "\n",
    "y_hat_matrix = X @ w + b\n",
    "y_hat_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4279a6e",
   "metadata": {},
   "source": [
    "### We have already seen this in Logistic Regression\n",
    "\n",
    "The forward pass is:\n",
    "\n",
    "- ğ‘§ = ğ‘‹ğ‘¤ + ğ‘\n",
    "- ğ‘¦^ = ğœ(ğ‘§)\n",
    "\n",
    "\n",
    "Same structure â€” just with a sigmoid on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1c3c8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62245933],\n",
       "       [0.92414182],\n",
       "       [0.98901306]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "z = X @ w + b\n",
    "y_hat_logistic = sigmoid(z)\n",
    "y_hat_logistic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a673c4e",
   "metadata": {},
   "source": [
    "### â€¦we can finally see the big idea:\n",
    "\n",
    "## A neural network layer is just a generalization of\n",
    "\n",
    "ğ‘‹ğ‘¤ + ğ‘\n",
    "\n",
    "Instead of one output, we compute h outputs:\n",
    "\n",
    "ğ‘‹ğ‘Š + ğ‘\n",
    "\n",
    "Where:\n",
    "\n",
    "ğ‘Š is (ğ‘‘ Ã— â„)\n",
    "\n",
    "output is \n",
    "\n",
    "(ğ‘› Ã— â„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91e0cdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2.1,  2.8, -2.2],\n",
       "        [ 5.1,  4.8, -4.2],\n",
       "        [ 8.1,  6.8, -6.2]]),\n",
       " (3, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X: (n=3 samples, d=2 features)\n",
    "X = np.array([\n",
    "    [1.0, 2.0],\n",
    "    [3.0, 4.0],\n",
    "    [5.0, 6.0]\n",
    "])\n",
    "\n",
    "# W: (d=2 features, h=3 outputs)\n",
    "W = np.array([\n",
    "    [1.0, -1.0,  0.5],\n",
    "    [0.5,  2.0, -1.5]\n",
    "])\n",
    "\n",
    "b = np.array([0.1, -0.2, 0.3])\n",
    "\n",
    "Z = X @ W + b\n",
    "Z, Z.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c3f1864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89090318, 0.94267582, 0.09975049],\n",
       "       [0.9939402 , 0.99183743, 0.01477403],\n",
       "       [0.99969655, 0.99888746, 0.00202532]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add an activation \n",
    "A = sigmoid(Z)\n",
    "A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6434e8",
   "metadata": {},
   "source": [
    "## Building a Real Design Matrix (US House Prices Dataset)\n",
    "**Now we connect everything to a real dataset.**\n",
    "\n",
    "I have a kaggle not book of this that uses US housing prices dataset.\n",
    "Weâ€™ll build a design matrix from these.\n",
    "\n",
    "Load the dataset EX: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be79acdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\n\\ndf = pd.read_csv(\"house_prices.csv\")\\ndf.head()\\n\\nfeatures = [\"sqft_living\", \"bedrooms\", \"bathrooms\"]\\ntarget = \"price\"\\n\\nX = df[features].values  # shape (n, d)\\ny = df[target].values.reshape(-1, 1)\\nX.shape, y.shape\\n\\n# Now that we have a design matrix **X** from the dataset\\n# This is how a neural network layer would consume it\\n\\n\\nn, d = X.shape      # n = number of examples, d = number of features\\nh = 3               # number of outputs (like 3 neurons in a layer)\\n\\n# Random weight initialization\\nW = np.random.randn(d, h) * np.sqrt(2.0 / d)\\n\\n# Bias vector\\nb = np.zeros(h)\\n\\nW.shape, b.shape\\n\\nZ = X @ W + b\\nZ[:5], Z.shape\\n\\ndef relu(z):\\n    return np.maximum(0, z)\\n\\nA = relu(Z)\\nA[:5]\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"house_prices.csv\")\n",
    "df.head()\n",
    "\n",
    "features = [\"sqft_living\", \"bedrooms\", \"bathrooms\"]\n",
    "target = \"price\"\n",
    "\n",
    "X = df[features].values  # shape (n, d)\n",
    "y = df[target].values.reshape(-1, 1)\n",
    "X.shape, y.shape\n",
    "\n",
    "# Now that we have a design matrix **X** from the dataset\n",
    "# This is how a neural network layer would consume it\n",
    "\n",
    "\n",
    "n, d = X.shape      # n = number of examples, d = number of features\n",
    "h = 3               # number of outputs (like 3 neurons in a layer)\n",
    "\n",
    "# Random weight initialization\n",
    "W = np.random.randn(d, h) * np.sqrt(2.0 / d)\n",
    "\n",
    "# Bias vector\n",
    "b = np.zeros(h)\n",
    "\n",
    "W.shape, b.shape\n",
    "\n",
    "Z = X @ W + b\n",
    "Z[:5], Z.shape\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "A = relu(Z)\n",
    "A[:5]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd210f50",
   "metadata": {},
   "source": [
    "# Final Summary: The Unifying Structure\n",
    "**Youâ€™ve now seen the entire descent:**\n",
    "\n",
    "- Singleâ€‘input linear regression\n",
    "- Multiâ€‘input dot product\n",
    "- Matrix form (Xw + b)\n",
    "- Logistic regression (sigmoid on top)\n",
    "- Neural network layer (XW + b)\n",
    "\n",
    "Real design matrix from real data\n",
    "\n",
    "**The entire field of supervised learning is built from:**\n",
    "\n",
    "- Linear transformation ğ‘‹ğ‘Š + ğ‘\n",
    "\n",
    "- Nonlinearity ğ‘“(â‹…)\n",
    "\n",
    "**Stack these two ideas and you get:**\n",
    "\n",
    "- regression\n",
    "- classification\n",
    "- deep neural networks\n",
    "- transformers\n",
    "- everything\n",
    "\n",
    "## This project completes your firstâ€‘principles foundation.\n",
    "\n",
    "**Preparing for Project 6**\n",
    "\n",
    "Now that youâ€™ve seen:\n",
    "\n",
    "- how X is built\n",
    "- how W and b are shaped\n",
    "- how XW + b works\n",
    "- how an activation is applied\n",
    "\n",
    "â€¦youâ€™re ready for the next project.\n",
    "\n",
    "## In Section 2 - Projectâ€¯6, you will replace this manual W and b with a real DenseLayer class that:\n",
    "\n",
    "- initializes weights\n",
    "- stores biases\n",
    "- performs forward passes\n",
    "- computes gradients\n",
    "- updates parameters through backpropagation\n",
    "\n",
    "And the design matrix X you built here will plug directly into that class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
