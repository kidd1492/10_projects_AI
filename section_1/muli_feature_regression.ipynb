{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3da076c5",
   "metadata": {},
   "source": [
    "# Project 2 â€” Multiâ€‘Feature Linear Regression (xâ‚, xâ‚‚ â†’ y)\n",
    "## Optimization Form â€” Gradient Descent (Multiâ€‘Feature)\n",
    "Same idea as Project 1, but now with two weights.\n",
    "\n",
    "## Scalar Gradient Descent (Project 1 style)\n",
    "\n",
    "-  The model becomes a plane ğ‘¦^ = ğ‘¤1ğ‘¥1 + ğ‘¤2ğ‘¥2 + ğ‘\n",
    " \n",
    "This is the simplest possible neural network:\n",
    "\n",
    "- inputs: \n",
    "ğ‘¥1, ğ‘¥2\n",
    "- weights: \n",
    "ğ‘¤1 ,ğ‘¤2\n",
    "- bias: ğ‘\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427ce7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Feature 1: square footage\n",
    "x1 = np.array([180, 200, 230, 260, 280, 300, 325, 375, 425, 480, 488, 510, 560, 600])\n",
    "\n",
    "# Feature 2: number of rooms\n",
    "x2 = np.array([4, 4, 5, 5, 6, 6, 6, 7, 7, 8, 8, 8, 9, 10])\n",
    "\n",
    "# Target: price\n",
    "y  = np.array([122, 120, 170, 180, 240, 238, 246, 320, 361, 370, 376, 390, 410, 470])\n",
    "\n",
    "# Combine into matrix\n",
    "X = np.column_stack([x1, x2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc889a1",
   "metadata": {},
   "source": [
    "## Understanding the Dot Product: \n",
    "### How Two Features Become One Prediction Line\n",
    "\n",
    "Before we run gradient descent, itâ€™s worth pausing to understand the core operation that makes\n",
    "multiâ€‘feature regression, and neural networks, work:\n",
    "\n",
    "**This dot product does something subtle but powerful:**\n",
    "\n",
    "ğ‘¤1x1 + w2x2 = wTx\n",
    "\n",
    "\n",
    "It takes two separate features and projects them onto a single line.\n",
    "\n",
    "That line is the direction defined by the weight vector \n",
    "ğ‘¤ = [ğ‘¤1,ğ‘¤2].\n",
    "\n",
    "This is why the model becomes:\n",
    "\n",
    "a plane in \n",
    "(ğ‘¥1,ğ‘¥2,ğ‘¦) space\n",
    "\n",
    "but a line in the transformed â€œdotâ€‘product spaceâ€\n",
    "\n",
    "Letâ€™s visualize that transformation stepâ€‘byâ€‘step.\n",
    "\n",
    "![image](mathematics/dot_product_vis.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4080b669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wieght 1 : 0.7737993926544017\n",
      "wieght 2 : 0.013236861842878735\n",
      "bias : -0.00444138868887303\n"
     ]
    }
   ],
   "source": [
    "# Scalar (non-vectorized) gradient descent\n",
    "w1 = 0.0\n",
    "w2 = 0.0\n",
    "b  = 0.0\n",
    "lr = 1e-7\n",
    "n = len(y)\n",
    "\n",
    "for epoch in range(20000):\n",
    "    y_hat = w1*x1 + w2*x2 + b\n",
    "    error = y_hat - y.flatten()\n",
    "\n",
    "    dw1 = (2/n) * np.sum(error * x1)\n",
    "    dw2 = (2/n) * np.sum(error * x2)\n",
    "    db  = (2/n) * np.sum(error)\n",
    "\n",
    "    w1 -= lr * dw1\n",
    "    w2 -= lr * dw2\n",
    "    b  -= lr * db\n",
    "\n",
    "print(f\"wieght 1 : {w1}\")\n",
    "print(f\"wieght 2 : {w2}\")\n",
    "print(f\"bias : {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db3eeb",
   "metadata": {},
   "source": [
    "# Vectorized Gradient Descent (Neuralâ€‘Network Style)\n",
    "**Updates all weights at once**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff13e5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wieght : [[0.77379939]\n",
      " [0.01323686]]\n",
      "bias : -0.004441388688873026\n"
     ]
    }
   ],
   "source": [
    "# Vectorized gradient descent\n",
    "X = np.column_stack([x1, x2])   # (n Ã— 2)\n",
    "y = y.reshape(-1, 1)            # (n Ã— 1)\n",
    "\n",
    "w = np.zeros((2, 1))            # (2 Ã— 1)\n",
    "b = 0.0\n",
    "\n",
    "lr = 1e-7\n",
    "n = len(y)\n",
    "\n",
    "for epoch in range(20000):\n",
    "    y_hat = X @ w + b           # (nÃ—2)(2Ã—1) â†’ (nÃ—1)\n",
    "    error = y_hat - y           # (nÃ—1)\n",
    "\n",
    "    dw = (2/n) * (X.T @ error)  # (2Ã—n)(nÃ—1) â†’ (2Ã—1)\n",
    "    db = (2/n) * np.sum(error)  # scalar\n",
    "\n",
    "    w -= lr * dw\n",
    "    b -= lr * db\n",
    "\n",
    "print(f\"wieght : {w}\")\n",
    "print(f\"bias : {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fea16a",
   "metadata": {},
   "source": [
    "One forward pass using a dot product\n",
    "\n",
    "One gradient vector\n",
    "\n",
    "One update rule\n",
    "\n",
    "This is literally how neural networks train\n",
    "The vectorized version is a neural network layer.\n",
    "\n",
    "Inputs â†’ dot product â†’ bias â†’ output â†’ gradient â†’ update.\n",
    "\n",
    "The only difference between this and a transformer is:\n",
    "\n",
    "more weights\n",
    "\n",
    "more layers\n",
    "\n",
    "nonlinear activations\n",
    "\n",
    "This is the moment where this naturally transitions into logistic regression, XOR, and backprop.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
